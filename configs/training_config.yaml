# Training Configuration for Neuro-Symbolic Code Generation AI

# Model Configuration
model:
  name: "deepseek-coder-6.7b"
  architecture: "transformer"
  hidden_size: 4096
  num_layers: 32
  num_heads: 32
  context_length: 16384
  vocab_size: 32000

# Training Parameters
training:
  batch_size: 16
  gradient_accumulation_steps: 4
  num_epochs: 3
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler:
    type: "cosine"
    num_cycles: 0.5
  
  # Optimizer
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    epsilon: 1.0e-8

# Distributed Training (DeepSpeed)
deepspeed:
  enabled: true
  config_file: "configs/deepspeed_config.json"
  zero_optimization:
    stage: 2
    offload_optimizer: false
    offload_param: false

# Data Configuration
data:
  train_data_path: "data/processed/train"
  val_data_path: "data/processed/validation"
  test_data_path: "data/processed/test"
  max_seq_length: 2048
  preprocessing:
    remove_comments: false
    normalize_whitespace: true
    filter_duplicates: true

# AST Encoder Configuration
ast_encoder:
  embedding_dim: 768
  hidden_dim: 1024
  num_layers: 6
  num_heads: 12
  max_depth: 32
  dropout: 0.1

# SMT Connector Configuration
smt_connector:
  solver_backend: "z3"
  timeout: 30
  max_iterations: 100
  enable_verification: true

# Reward Model Configuration
reward_model:
  test_weight: 0.6
  security_weight: 0.15
  quality_weight: 0.15
  license_weight: 0.1
  normalize: true

# Logging and Checkpointing
logging:
  log_level: "INFO"
  log_interval: 100
  eval_interval: 500
  save_interval: 1000
  log_dir: "logs/training"
  
checkpoint:
  save_dir: "models/checkpoints"
  keep_last_n: 3
  save_optimizer_state: true

# MLflow Tracking
mlflow:
  enabled: true
  tracking_uri: "http://localhost:5000"
  experiment_name: "neurosymbolic-codegen"
  run_name: "training-run-{timestamp}"

# Early Stopping
early_stopping:
  enabled: true
  patience: 3
  metric: "val_loss"
  mode: "min"

# Mixed Precision Training
mixed_precision:
  enabled: true
  fp16: false
  bf16: true

# Evaluation
evaluation:
  benchmarks: ["humaneval", "mbpp", "codecontests"]
  num_samples: 100
  temperature: 0.2
  top_p: 0.95
  max_new_tokens: 512
