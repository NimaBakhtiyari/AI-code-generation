apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: neurosymbolic-finetune-
  labels:
    app: neurosymbolic-codegen
    workflow-type: finetuning
spec:
  entrypoint: finetune-pipeline
  
  arguments:
    parameters:
    - name: base-model
      value: "deepseek-coder-6.7b"
    - name: use-lora
      value: "true"
    - name: lora-rank
      value: "16"
    - name: lora-alpha
      value: "32"
    - name: target-modules
      value: "q_proj,v_proj,k_proj,o_proj"
  
  volumeClaimTemplates:
  - metadata:
      name: workspace
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 100Gi
  
  templates:
  - name: finetune-pipeline
    steps:
    - - name: setup
        template: setup-env
    - - name: load-base-model
        template: load-model
        arguments:
          parameters:
          - name: model-name
            value: "{{workflow.parameters.base-model}}"
    - - name: configure-peft
        template: setup-peft
        arguments:
          parameters:
          - name: use-lora
            value: "{{workflow.parameters.use-lora}}"
          - name: lora-rank
            value: "{{workflow.parameters.lora-rank}}"
          - name: lora-alpha
            value: "{{workflow.parameters.lora-alpha}}"
          - name: target-modules
            value: "{{workflow.parameters.target-modules}}"
    - - name: finetune
        template: run-finetuning
    - - name: merge-adapters
        template: merge-lora-weights
        when: "{{workflow.parameters.use-lora}} == true"
    - - name: save-model
        template: save-finetuned-model
  
  - name: setup-env
    container:
      image: python:3.11-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y curl git
          curl -sSL https://install.python-poetry.org | python3 -
          export PATH="/root/.local/bin:$PATH"
          cd /workspace
          poetry install
      volumeMounts:
      - name: workspace
        mountPath: /workspace
  
  - name: load-model
    inputs:
      parameters:
      - name: model-name
    container:
      image: python:3.11-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          pip install huggingface-hub
          huggingface-cli download {{inputs.parameters.model-name}} \
            --local-dir /workspace/models/pretrained/{{inputs.parameters.model-name}}
      volumeMounts:
      - name: workspace
        mountPath: /workspace
  
  - name: setup-peft
    inputs:
      parameters:
      - name: use-lora
      - name: lora-rank
      - name: lora-alpha
      - name: target-modules
    container:
      image: python:3.11-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          cd /workspace
          cat > configs/peft_config.json << EOF
          {
            "peft_type": "LORA",
            "task_type": "CAUSAL_LM",
            "r": {{inputs.parameters.lora-rank}},
            "lora_alpha": {{inputs.parameters.lora-alpha}},
            "target_modules": "{{inputs.parameters.target-modules}}".split(","),
            "lora_dropout": 0.05,
            "bias": "none"
          }
          EOF
          echo "PEFT config created"
      volumeMounts:
      - name: workspace
        mountPath: /workspace
  
  - name: run-finetuning
    container:
      image: nvcr.io/nvidia/pytorch:23.10-py3
      command: ["/bin/bash", "-c"]
      args:
        - |
          cd /workspace
          poetry run python -m neurosymbolic_codegen.finetune \
            --config configs/finetune_config.yaml \
            --peft-config configs/peft_config.json \
            --output-dir /workspace/models/finetuned
      resources:
        limits:
          nvidia.com/gpu: "2"
      volumeMounts:
      - name: workspace
        mountPath: /workspace
  
  - name: merge-lora-weights
    container:
      image: python:3.11-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          cd /workspace
          poetry run python -c "
          from peft import PeftModel
          from transformers import AutoModelForCausalLM
          
          base_model = AutoModelForCausalLM.from_pretrained('/workspace/models/pretrained')
          model = PeftModel.from_pretrained(base_model, '/workspace/models/finetuned/lora_adapter')
          merged_model = model.merge_and_unload()
          merged_model.save_pretrained('/workspace/models/finetuned/merged')
          print('LoRA weights merged successfully')
          "
      volumeMounts:
      - name: workspace
        mountPath: /workspace
  
  - name: save-finetuned-model
    container:
      image: python:3.11-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          cd /workspace
          tar -czf finetuned-model.tar.gz models/finetuned/
          echo "Finetuned model saved"
      volumeMounts:
      - name: workspace
        mountPath: /workspace
    outputs:
      artifacts:
      - name: finetuned-model
        path: /workspace/finetuned-model.tar.gz
